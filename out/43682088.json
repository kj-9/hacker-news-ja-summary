{
  "comments_id": "43682088",
  "rank": 3,
  "title": "The path to open-sourcing the DeepSeek inference engine",
  "link": "https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine",
  "created_date": "2025-04-14T21:27:57.549314",
  "comments_summary": "この記事は、DeepSeekがvLLMのフォークを公開することについてのHacker Newsのコメントをまとめたものです。DeepSeekは、DeepSeekモデル用にvLLMを大幅にカスタマイズしましたが、その結果、保守が困難になりました。\n\n## コードベースの分岐と保守性\n\nDeepSeekのエンジンは、1年以上前のvLLMの初期フォークに基づいています。彼らはDeepSeekモデル用にそれを大幅にカスタマイズしたため、より広範なユースケースに拡張することが困難になっています。vLLMは大きく変更されているため、DeepSeekはコミュニティの変更を移植することがますます困難になっています。彼らはハードウェアから信じられないほどのパフォーマンスを引き出す最適化を行ってきましたが、特定の機械構成も持っています。\n\n## 公開のメリット\n\nDeepSeekは、最適化を公開するために時間と労力を費やしています。彼らは変更を内部に保持するよりも、公開することを選択しました。実行不可能なコードでも非常に役立ちます。テキストや方程式だけでは十分に具体的ではないため、実際に何をしたかを確認するためだけに、一部の論文で利用可能になることを願うことがあります。\n\n## パフォーマンスの向上\n\n3月には、vLLMはDeepSeek論文の改善点をいくつか取り入れました。これにより、vLLM v0.7.3のDeepSeekパフォーマンスは約3倍以上に向上しました。DeepSeek-V3/R1推論システム概要では、「各H800ノードは、プリフィル中に平均73.7kトークン/秒の入力（キャッシュヒットを含む）、またはデコード中に14.8kトークン/秒の出力を実現します」と引用されています。\n\n## 動機\n\n商業AI企業が研究結果やノウハウを共有する動機は何でしょうか？GoogleがTransformerアーキテクチャを公開した理由は何でしょうか？企業経営陣は、商業的利益に反する行為をどのように受け入れ、株主からの異議を回避できるのでしょうか？人々があなたの時代遅れのものをコピーするほど、彼らは常にあなたに遅れをとるので、あなたにとって有利になります。\n\n## まとめ\n\nDeepSeekがvLLMのフォークを公開することは、コミュニティにとって良いことです。彼らの最適化はパフォーマンスを向上させ、他の人が学ぶことができます。商業AI企業が研究結果やノウハウを共有する動機は複雑ですが、コミュニティ全体にとって有益です。"
}