{
  "comments_id": "44149238",
  "rank": 5,
  "title": "Why DeepSeek is cheap at scale but expensive to run locally",
  "link": "https://www.seangoedecke.com/inference-batching-and-deepseek/",
  "created_date": "2025-06-01T20:12:20.722707",
  "comments_summary": "このHacker Newsのスレッドは、DeepSeekなどの大規模言語モデル（LLM）のコスト効率、推論方法、ハードウェア要件、および潜在的なセキュリティリスクについて議論しています。\n\n## DeepSeekの費用対効果\n\nDeepSeekは、その競合他社と比較して費用対効果が高いと評価されています。しかし、一部のユーザーは、DeepSeekのAPIを通じてアクセスすると、Google Gemini Proと同様に、タスクを完了するために必要なすべてのコードを生成するのではなく、パッチファイルを提供するなど、効率化のために「怠惰」になることがあると報告しています。ただし、DeepSeekはオープンソースであるため、ユーザーは独自のハードウェア上でモデルをホストし、サードパーティアプリケーションに依存することなく、好みのバージョンを使用できます。\n\n## バッチ推論と非決定性\n\nLLMプロバイダーは、複数のユーザーのプロンプトをバッチ処理して、モデルインスタンスを効率的に利用しています。これにより、温度を0に設定しても、毎回異なる応答が得られる可能性があります。これは、他のプロンプトとのバッチ処理が結果に影響を与えるためです。バッチ処理はパフォーマンスを向上させますが、非決定性や潜在的なデータ漏洩のリスクも伴います。\n\n## ハードウェア要件と最適化\n\n大規模なLLMをローカルで実行するには、大量のメモリと計算能力が必要です。GPUは一般的ですが、CPUベースのセットアップも可能です。特に、十分なRAMを備えたEPYCベースのサーバーを使用すると、GPUを使用せずにDeepSeekをローカルで実行できます。MoE（Mixture of Experts）モデルは、計算ニーズが低いため、Apple Siliconなどの統合メモリアーキテクチャに最適です。ソフトウェアの最適化は、特にメモリ容量が限られているシステムでパフォーマンスを向上させるために重要です。\n\n## まとめ\nこのスレッドでは、LLMの費用対効果、推論方法、ハードウェア要件、および潜在的なセキュリティリスクについて議論されています。DeepSeekは費用対効果が高いものの、APIの使用にはいくつかの課題があります。バッチ推論は効率的な方法ですが、非決定性やデータ漏洩のリスクがあります。LLMをローカルで実行するには、適切なハードウェアとソフトウェアの最適化が必要です。"
}