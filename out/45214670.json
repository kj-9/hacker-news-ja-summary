{
  "comments_id": "45214670",
  "rank": 1,
  "title": "Top model scores may be skewed by Git history leaks in SWE-bench",
  "link": "https://github.com/SWE-bench/SWE-bench/issues/465",
  "created_date": "2025-09-11T20:10:30.920463",
  "comments_summary": "Hacker Newsのスレッドでは、LLM（大規模言語モデル）のコード生成ベンチマーク「SWE-bench」の検証プロセスや、その結果の信頼性について議論されています。特に、ベンチマークの「Verified（検証済み）」というラベルが、実際にはモデルが問題を解く際にgitの履歴から答えを「カンニング」できる状態であったことが指摘されています。\n\n## SWE-benchの検証プロセスとデータの汚染\n\n「SWE-Bench Verified」の検証プロセスについて、実際には過去の「SWE-Bench」に存在した、ベンチマークとして適切でないタスクを修正したことを指すという意見があります。データ汚染の問題は、モデルのテストに厳しい制限を設けることでしか対処できないと指摘されています。ベンチマーク問題が専門家によって検証された事実は、モデルのデータ汚染とは無関係であるという意見もあります。\n\n## LLMベンチマークの信頼性\n\nLLMベンチマークの結果に対する懐疑的な意見が多くあります。特に、SOTA（最先端）モデルが驚くほど単純な問題で失敗する例が挙げられ、LLMがコードを理解しているかのような錯覚から人々を目覚めさせるという意見があります。コーディングベンチマークが実際の開発経験と一致していないという不満や、特定のユースケースに特化したベンチマークが必要であるという指摘もあります。\n\n## LLMの性能と現実のギャップ\n\nTerminal-Benchのような他のベンチマークについても、現実のLLMの性能とはかけ離れた結果が出ているという指摘があります。また、SWE-benchにおいて、モデルがgitの履歴から答えをカンニングできる状態であったにもかかわらず、100%の正答率を達成できていないことは、SOTAモデルの性能に対する厳しい評価であるという意見もあります。\n\n## まとめ\n\nこのスレッドでは、LLMのコード生成ベンチマークの検証プロセスの不備や、その結果の信頼性に対する疑問が提起されています。特に、SWE-benchにおけるデータ汚染の問題や、LLMがgitの履歴から答えを「カンニング」できる状態であったことが問題視されています。また、ベンチマークの結果が現実のLLMの性能と乖離しているという指摘や、LLMに対する過度な期待に対する警鐘も鳴らされています。"
}