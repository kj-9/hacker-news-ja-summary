{
  "comments_id": "44971845",
  "rank": 8,
  "title": "Weaponizing image scaling against production AI systems",
  "link": "https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/",
  "created_date": "2025-08-21T20:12:49.954302",
  "comments_summary": "このHacker Newsのスレッドでは、画像に隠されたテキストを利用したLLM（大規模言語モデル）への新しい攻撃手法について議論されています。\n\n## LLMのセキュリティ上の脆弱性\n\n- LLMは、ユーザー入力とシステム入力を区別する能力に限界があり、セキュリティ境界を侵害される可能性がある。\n- 従来のプログラムのように、ユーザー入力を制限することが難しい。\n- LLMは本質的に非決定的であるため、セキュリティプロパティを持たない。\n\n## プロンプトインジェクション攻撃の可能性\n\n- 画像にレンダリングされたテキストをLLMが読み取り、それを信頼してしまう可能性がある。\n- ダウンスケーリング時に異なる画像に見えるように画像を操作することで、攻撃を隠蔽できる。\n- LLMが外部ソースからのコマンドを区別できないことが根本的な問題である。\n\n## 対策の難しさ\n\n- モデルのファインチューニングだけで攻撃を防ぐことは難しい。\n- データとプロンプトを分離することがLLMの構造上難しい。\n- 人間が入力/出力を検証することに依存すると、隠された情報による攻撃を防ぐことができない。\n\n## まとめ\n\nこのスレッドでは、LLMのセキュリティ上の脆弱性と、画像に隠されたテキストを利用した新しい攻撃手法の可能性が指摘されています。LLMはユーザー入力とシステム入力を区別する能力に限界があり、攻撃者がLLMの動作を制御する可能性があります。また、既存の対策は完全ではなく、新たな攻撃手法が現れる可能性も指摘されています。この問題は、AIシステムのセキュリティを確保するために早急に解決する必要がある重要な課題です。"
}