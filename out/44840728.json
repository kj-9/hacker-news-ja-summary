{
  "comments_id": "44840728",
  "rank": 6,
  "title": "Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?",
  "link": "https://news.ycombinator.com/item?id=44840728",
  "created_date": "2025-08-08T20:13:45.654057",
  "comments_summary": "大規模なLLM推論を可能にするエンジニアリングの工夫について議論しています。\n\n## スケールとリソース\n* OpenAIは、週に約7億人のユーザーをChatGPTで処理しており、大規模なGPUクラスタを所有しています。\n* ハードウェア(GPU)にお金をかければ解決できるという意見もあります。\n* 大規模なGPUクラスタには、「莫大なVRAM」が搭載されています。オフロードなしで実際にモデルをロードできれば、ほとんどの場合、推論はそれほど計算コストがかかりません。\n* OpenAIはMicrosoft Azureのクラウドインフラストラクチャでホストされており、Microsoftは49%を所有しています。\n\n## 最適化と手法\n* モデルの最適化、シャーディング、カスタムハードウェア、巧妙なロードバランシング、プロンプトのキャッシング、RAG、CUDAの最適化、スロットリング、推論時の計算以外の他のアプローチを使用したオフロードなどが行われています。\n* 推論はバッチ処理できるため、複数のクエリを並列バッチでまとめて処理する方が、ユーザーごとにGPUを1つずつインタラクティブに割り当てるよりも効率的です。\n* 投機的デコーディングは、より小さいドラフトモデルを使用して、はるかに少ない計算量とメモリでトークンを生成します。次に、メインモデルは、生成したであろう確率に基づいてそれらのトークンを受け入れます。\n* 構造化された出力のための別のトリックは、「高速転送」です。JSONを生成するときに`{ \"\": `などで始める必要があることがわかっている場合、トークンをスキップできます。\n* バッチ処理はレイテンシを増加させるため、トレードオフが生じます。MoEは均等に使用されないため、さらに複雑になります。\n\n## ハードウェア\n* GoogleはTPUを持っており、Nvidiaカードよりも効率的です。\n* Positron.aiは、専用ハードウェアによるLLMワークロードのスケーリングへのアプローチについて議論しています。\n* CerebrasとGroqは、すでに推論のスループットが非常に高くなっています。AppleがAppleシリコンAIサーバーを開発する場合も面白いかもしれません。\n\n## まとめ\nこのディスカッションでは、大規模なLLM推論を可能にするさまざまな要因について議論しています。主な要因は、リソース（大規模なGPUクラスタ）と、モデルの最適化、シャーディング、カスタムハードウェアなどのエンジニアリング上の工夫です。投機的デコーディングやバッチ処理などの手法も、パフォーマンスを向上させるために使用されています。また、TPUやCerebrasなどの専用ハードウェアも、LLM推論の効率を高める上で重要な役割を果たしています。"
}