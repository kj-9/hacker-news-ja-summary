{
  "comments_id": "44053744",
  "rank": 8,
  "title": "LLM function calls don't scale; code orchestration is simpler, more effective",
  "link": "https://jngiam.bearblog.dev/mcp-large-data/",
  "created_date": "2025-05-21T20:13:07.692484",
  "comments_summary": "LLM（大規模言語モデル）がタスクを実行する際の課題と、その解決策についての議論\n\n## 実行環境と状態管理\n\nLLMの実行環境は通常ステートフルであり、Jupyterカーネルのようなものがユーザーセッションごとに必要となる。これは管理が難しく、コストもかかるため、ステートレスで永続的な実行環境が重要となる。イベントソーシングやDurable ExecutionといったアーキテクチャパターンがAIの実行において重要になるが、AIフレームワークでは一般的ではない。\n\n## エラー処理と継続実行\n\nLLMが生成したコードの実行が途中で失敗した場合、エラー発生時の変数の状態から処理を再開できることが望ましい。LLMは正しく再開できるコードを生成できるが、それを可能にするランタイムの構築が難しい。APIをLLMに対して冪等にすることで、LLMがアクション全体をリプレイするか、失敗したステップだけをリプレイするかをバックエンドが気にしなくてもよくなる。\n\n## LLMと決定論的アプローチのハイブリッド\n\n決定論的なアプローチを最大限に活用し、残りの複雑な部分をLLMで処理するハイブリッドな解決策が最適である。LLMを使用して決定論的なアプローチ（コード）を生成し、それがうまく機能すれば、将来のために保存して決定論的に使用する。また、決定論的な方法を使用して、LLMへの最適な入力を生成することもできる。\n\n## コンテキストメモリのインデックス化\n\nモデルがspanをstart:endの範囲で参照できるようにすることで、明示的な引用の代わりに参照渡しで引数を渡せるようにする。これらのspanを抽出QAタスクの回答やコードブロックの引数として使用したり、ポインタからグラフを構築してグラフ計算を実行したりできる。\n\n## まとめ\n\nLLMの利用における課題（コスト、速度、エラー処理）と、その解決策（ステートレス実行環境、冪等なAPI、決定論的アプローチとの組み合わせ、コンテキストメモリのインデックス化）について議論されている。LLMの能力を最大限に引き出しつつ、効率的かつ信頼性の高いシステムを構築するための様々なアプローチが提案されている。"
}