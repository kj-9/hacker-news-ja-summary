{
  "comments_id": "44856101",
  "rank": 3,
  "title": "Diffusion language models are super data learners",
  "link": "https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac",
  "created_date": "2025-08-10T20:13:23.852809",
  "comments_summary": "このHacker Newsのスレッドでは、拡散モデル（DLM）と自己回帰モデル（AR）の比較に関する論文について議論しています。\n\n## FLOPsの増加\n\n拡散モデルは自己回帰モデルと比較して、推論時に必要なFLOPsが大幅に増加します。これは、拡散モデルが自己回帰的ではなく、Key Value（KV）キャッシュを使用できないためです。KVキャッシュは、自己注意メカニズムにおける計算量を削減するために使用されます。\n\n## 評価指標の信頼性\n\n検証損失を異なるアーキテクチャの比較に使用することの妥当性について疑問が呈されています。損失は、測定したいものを直接測定するのではなく、その代理として機能するため、アーキテクチャによって損失の状況が大きく異なる場合、信頼性が低い可能性があります。\n\n## モデルの記憶容量\n\n拡散モデルは自己回帰モデルよりも記憶容量が低い可能性があります。自己回帰モデルは、同じ数の学習トークンに対してより良い損失を示します。\n\n## 計算リソースの重要性\n\n拡散モデルは自己回帰モデルよりも多くの計算リソースを必要とします。同じ重みでより多くの計算を行うことで、より良い結果が得られる可能性があります。拡散モデルと自己回帰モデルを比較する際には、重みだけでなく計算量も一致させる必要があります。\n\n## まとめ\n\nこのスレッドでは、拡散モデルと自己回帰モデルの比較に関する論文について、FLOPsの増加、評価指標の信頼性、モデルの記憶容量、計算リソースの重要性などの側面から議論されています。参加者は、論文の主張を検証するために、より詳細な情報や追加の分析が必要であると指摘しています。"
}