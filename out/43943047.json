{
  "comments_id": "43943047",
  "rank": 9,
  "title": "Vision Now Available in Llama.cpp",
  "link": "https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md",
  "created_date": "2025-05-10T20:11:34.825082",
  "comments_summary": "llama.cppがvisionサポートを再開し、llama-server GUIにも追加されたことについてのHacker Newsのスレッドです。\n\n## llama.cppのVisionサポート\n*   llama.cppでvisionサポートが復活し、llama-server GUIでも利用可能になった。\n*   これにより、ローカルLLMで画像に関するタスクを実行できるようになった。\n*   以前はvisionサポートがdeprecatedされていたが、再実装された。\n*   llama.cppは複数のプラットフォーム向けにコンパイル済みのリリースを提供している。\n*   llama-mtmd-cliプログラムを使用して、画像とチャットできる。\n\n## 実行速度と最適化\n*   llama.cppはollamaよりも高速に実行できるように最適化されている。\n*   ngxson氏がllama.cppのvisionサポートに大きく貢献している。\n*   MetalバックエンドではGPUオフロードがデフォルトで有効になった。\n*   Gemma 3などのモデルで、キーワードや説明の生成に活用できる。\n\n## モデルと利用方法\n*   Gemma 3 4bなどのモデルが、画像の説明に利用されている。\n*   SmolVLMシリーズは、リアルタイムなホームビデオ監視システムに適している。\n*   UnslothのGGUF quantを使用すると、開発中に非常にうまく動作する。\n*   llama-mtmd-cliを使用すると、GGUF形式のモデルを簡単に実行できる。\n\n## まとめ\nllama.cppがvisionサポートを再開したことで、ローカルLLMの可能性が広がりました。特に画像処理タスクにおいて、その高速性と最適化により、さまざまなアプリケーションでの利用が期待されます。コミュニティの貢献により、着実に進化している点が評価されています。"
}