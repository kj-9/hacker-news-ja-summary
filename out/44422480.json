{
  "comments_id": "44422480",
  "rank": 5,
  "title": "Show HN: TokenDagger – A tokenizer faster than OpenAI's Tiktoken",
  "link": "https://github.com/M4THYOU/TokenDagger",
  "created_date": "2025-06-30T20:13:18.883239",
  "comments_summary": "TokenDaggerは、OpenAIのTiktokenのドロップイン代替品であり、Llama 3、Mistral、GPT-3などのトークナイザーです。C++ 17で書かれており、Pythonバインディングが薄く、BPE語彙/特別なトークンルールをそのまま保持し、生の速度に重点を置いています。TikTokenのPython/Rust実装をプロファイリングすると、正規表現のマッチングに多くの時間が費やされていることがわかりました。パフォーマンスの向上は、a) より高速なjitコンパイルされた正規表現エンジンを使用すること、b) アルゴリズムを簡略化して特別なトークンでの正規表現マッチングを完全に回避することによって得られました。\n\n## パフォーマンスの改善とC++の利点\n\n多くの人が、パフォーマンスを大幅に向上させるようなドロップイン代替品を作成することの素晴らしさを指摘しています。AI/MLインフラストラクチャの重要なボトルネックをC++でコーディングすることで、パフォーマンス最適化が実現できます。特に、中国のエンジニアがC++で実装したものは、エンジニアリング上のトレードオフがうまく行われており、劇的な改善が見られることが多いです。\n\n## 開発の原則と段階\n\nソフトウェア開発の段階について、「Make it work, make it fast, make it pretty」という原則が紹介されています。現在、トランスフォーマーとLLMは十分に機能する段階にあり、パフォーマンス面での進歩が重要視されています。別の言い方として、「Make It Work, Make It Right, Make It Fast」というものもあり、「Make it right」は、保守性やテスト容易性を考慮してリファクタリングや書き換えを行うことを意味します。\n\n## PythonとC++の役割\n\nPythonから離れるべきかという議論では、研究におけるイテレーションの速さが重要であり、PythonはGPU操作のオーケストレーションに適しているという意見があります。ただし、パフォーマンスが重要なコードはCやCythonで記述されることが多く、NumPy、Scikit-learn、Pandasなどが例として挙げられています。一方、C++で書き直す場合、CUDAカーネルをより効率的に書き換えることが主な目的となります。\n\n## 特殊トークンの処理と品質\n\nTokenDaggerでは、特殊トークンの正規表現マッチングを回避するために、アルゴリズムが簡略化されています。これにより、初期化時にすべての特殊トークンを正規表現にコンパイルするTiktokenの実装よりも高速になります。TokenDaggerは、string matchingを使用し、`encode`の呼び出し元が考慮すべき特殊トークンを明示的に定義することで、パフォーマンスを向上させています。\n\n## その他のトークナイザーと将来の方向性\n\n他のLLMのローカルトークナイザーに関する議論があり、GeminiがSentencePieceを使用していることが指摘されています。TokenDaggerは、増分再トークン化に取り組んでおり、他のクレートとのベンチマークも予定しています。また、モデル固有の特殊な事情をライブラリに組み込むことで、パフォーマンスの向上と統合の容易化が期待されています。\n\n## 全体的なパフォーマンスへの影響\n\nLLMのパフォーマンスにおいてトークナイザーの最適化がどれほど重要かという質問に対して、テキストのトークン化は全体的な計算のごく一部であるという意見があります。ただし、ペタバイト単位のデータを扱う場合には、高速なトークナイザーが役立ちます。また、トークン化は通常CPUで行われ、トレーニングや推論のボトルネックになることはまれであり、GPUが現在のバッチを処理している間にCPUが次のバッチを準備することで、トークン化のレイテンシを隠すことができます。\n\n## まとめ\n\nTokenDaggerは、Tiktokenの高速なドロップイン代替品として、特に正規表現マッチングの最適化によってパフォーマンスを向上させています。C++を使用することで、AI/MLインフラストラクチャのボトルネックを解消し、全体的な効率を高めることができます。今後の開発では、増分再トークン化やモデル固有の特殊事情への対応が進められる予定です。"
}