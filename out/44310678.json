{
  "comments_id": "44310678",
  "rank": 5,
  "title": "Show HN: I built a tensor library from scratch in C++/CUDA",
  "link": "https://github.com/nirw4nna/dsc",
  "created_date": "2025-06-18T20:14:10.934197",
  "comments_summary": "nirw4nnaが、C++/CUDAでテンソルライブラリ`dsc`を構築し、ローカルで小さなLLMを実行するためのクリーンなAPI、シンプルさ、明確な可観測性を優先していることを紹介しています。PythonのようなAPIを持ち、Qwenのようなモデルをロードして推論を実行することができます。今後のロードマップにはBF16のサポートとGPUワークロードの可視化が含まれています。\n\n## 中間表現とコンパイラのようなアーキテクチャ\nテンプレートとスイッチ文は、中間表現とコンパイラのようなアーキテクチャがあれば、より良くなるのではないかと考えています。\n\n## Jaxやzmlとの比較\nこれがJaxやzmlのようなものとどのように比較されるのか疑問に思っています。\n\n## プロジェクトの目標\nこのプロジェクトの目標は、個人的な学習、推論パフォーマンス、またはその他のものでしょうか？また、推論速度がllama.cppと比較してどうなのか知りたいです。\n\n## cublasの使用\nどちらも内部ではcublasを使用しています。したがって、プリフィルについては同様だと思います（もちろん、このフレームワークはまだ初期段階であり、GEMMのFP16/BF16サポートはないようです）。手作りのgemvはトークン生成には高速であるため、llama.cppの方が優れています。\n\n## ctypesインターフェース\nctypesを介してネイティブコードとインターフェースしていることに気づきました。一般的にcffiの方が推奨されます。独自のPython拡張モジュール（pybindを使用するなど）を構築すれば、より柔軟性が高まります。CとPythonの厳格な分離は意図的な設計上の選択だったのか疑問に思っています。\n\n## シリアライゼーションとデシリアライゼーション\nテンソルとnnライブラリのシリアライゼーションとデシリアライゼーションの計画はありますか？\n\n## まとめ\nnirw4nnaは、C++/CUDAで構築されたテンソルライブラリ`dsc`を紹介しました。このライブラリは、クリーンなAPI、シンプルさ、可観測性に重点を置いており、PythonのようなAPIを持ち、Qwenのようなモデルをロードして推論を実行することができます。コメントでは、中間表現、Jaxやzmlとの比較、プロジェクトの目標、cublasの使用、ctypesインターフェース、シリアライゼーションなど、さまざまな側面について質問や提案が寄せられています。"
}