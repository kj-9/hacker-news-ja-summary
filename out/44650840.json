{
  "comments_id": "44650840",
  "rank": 5,
  "title": "Subliminal Learning: Models Transmit Behaviors via Hidden Signals in Data",
  "link": "https://alignment.anthropic.com/2025/subliminal-learning/",
  "created_date": "2025-07-22T20:14:20.348813",
  "comments_summary": "Hacker Newsのスレッドでは、LLM（大規模言語モデル）における予想外の知識伝達に関する最近の研究について議論されています。この研究では、あるモデルの好みが、表面上は無関係なタスクを実行する際に、別のモデルに「subliminally（潜在的に）」伝達されることが示されています。\n\n## モデルの相互接続性と一般化\nこの研究結果は、LLMの内部における概念的な相互接続性が予想以上に高い可能性を示唆しています。また、モデルがトークンを選択するメカニズムが、より一般化されており、トレーニングデータの影響を受けやすい可能性も示唆しています。このことは、LLMの「知能」が人間のモデルとは大きく異なることを意味するかもしれません。\n\n## スプリアスなつながりと因果関係\nモデルは、人間が関連性がないと考えるオブジェクト間で、スプリアスなつながりを持つことがあります。この研究は、そのつながりがどのように現れるかを示しています。モデルの出力（乱数など）は、出力とは無関係であるべき内部要因を反映している可能性があります。完全に接続されたモデルの性質は、現実を反映しない相互作用を引き出すコンテキストが常に存在する可能性があることを意味します。\n\n## モデル間の知識伝達\nあるモデルの好みが別のモデルに伝達されるという事実は、AIの安全性の問題を引き起こす可能性があります。以前のLLMの出力で新しいLLMをトレーニングすると、以前のモデルの好みも伝達される可能性があります。ただし、この効果は同一のモデル（同じアーキテクチャと初期化）を必要とするため、次世代モデルのトレーニングでは発生しない可能性があります。\n\n## まとめ\nこの研究は、LLMの内部動作と、モデル間の知識伝達の可能性について、興味深い洞察を提供します。ただし、Anthropicの研究に対する擬人化された表現は、いくつかの批判を集めています。全体として、この研究はLLMの複雑さを浮き彫りにし、AIの安全性と解釈可能性に関する重要な問題を提起しています。"
}