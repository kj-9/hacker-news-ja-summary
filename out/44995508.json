{
  "comments_id": "44995508",
  "rank": 8,
  "title": "Writing Speed-of-Light Flash Attention for 5090 in CUDA C++",
  "link": "https://gau-nernst.github.io/fa-5090/",
  "created_date": "2025-08-23T20:11:48.635510",
  "comments_summary": "この記事は、CUDA C++でAttentionメカニズムを実装することについて議論しています。特に、Tritonでは利用できないsm120向けのMXFP8 / NVFP4 MMAなどの機能を利用するために、CUDA C++を使用するモチベーションが述べられています。\n\n## パフォーマンスと価格\n* 5090のBF16 TFLOPsは理論上209.5であり、これはサーバー向けのBlackwell（B200は2250、GB200は2500）の10%にも満たない。\n* B200の価格は約30,000〜40,000ドル/GPUであるため、性能/ドルで比較すると、5090はお得とは言えない可能性がある。\n* NVIDIAは4090以降、ゲーミングカードのテンソルコアの性能を制限しており、特にMLトレーニングで使用される可能性のある演算において制限がある。\n* FP8およびFP16のmatmulは、FP16で累積する場合はフルスピードで実行されるが、FP32で累積する場合は半分のスピードでしか実行されない。\n* ワークステーションクラスのカード（RTX Pro 6000など）では、この制限は解除されている。\n\n## ハードウェアとソフトウェアのサポート\n* PytorchはBlackwellアーキテクチャをネイティブにサポートしているが、パフォーマンスはHopperよりも悪いという意見がある。\n* 5090でFlash Attentionがネイティブに動作するかどうかについての質問があり、適切なアーキテクチャフラグをnvccに渡すことでコンパイルできるはずだが、新しいハードウェア機能を活用できるとは限らない。\n\n## その他の考慮事項\n* 5090は4090よりもTDPが高く、電力制限も70%までしかできないため、ワークステーションでのML用途には不向きという意見がある。\n\n## まとめ\nこの記事では、CUDA C++でのAttentionメカニズムの実装、5090のパフォーマンスと価格、ハードウェアとソフトウェアのサポート、その他の考慮事項について議論されています。5090はメモリ帯域幅は優れているものの、性能/ドルで比較するとB200ほどお得ではない可能性があり、ワークステーションでのML用途にはいくつかの制約があるようです。"
}